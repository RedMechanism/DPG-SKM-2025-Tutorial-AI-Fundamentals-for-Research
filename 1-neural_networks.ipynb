{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414708e7",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial: AI Fundamentals for Research (joint session BP/TUT/DY/AKPIK)\n",
    "Sunday, 16th March 2025, 16:00–18:15, H2<br>\n",
    "Presented at the **[2025 DPG Spring Meeting of the Condensed Matter Section](https://www.dpg-verhandlungen.de/year/2025/conference/regensburg/part/tut/session/1)**\n",
    "\n",
    "## Hands-On Session 1: Function Approximation\n",
    "#### •Jan Bürger<sup>1</sup>, Janine Graser<sup>2</sup>, Robin Msiska<sup>2,3</sup>, and Arash Rahimi-Iman<sup>4</sup>\n",
    "\n",
    "<sup>1</sup>ErUM-Data-Hub, RWTH Aachen University, Aachen, Germany<br>\n",
    "<sup>2</sup>Faculty of Physics and Center for Nanointegration Duisburg-Essen (CENIDE), University of Duisburg-Essen, Duisburg, Germany<br>\n",
    "<sup>3</sup>Department of Solid State Sciences, Ghent University, Ghent, Belgium<br>\n",
    "<sup>4</sup>I. Physikalisches Institut and Center for Materials Research, Justus-Liebig-University Gießen, Gießen, Germany\n",
    "\n",
    "### Abstract\n",
    "*In the first half of the interactive session, participants will work with Jupyter Notebooks to explore practical applications of machine learning. They will train simple neural networks to predict a mathematical function, gaining hands-on experience in tuning key parameters. Since neural networks can typically be considered universal function approximators, this concept is effectively illustrated using a one-dimensional function, making it easy to visualize and understand.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef66c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### About this notebook\n",
    "#### **Presenter: Jan Bürger**\n",
    "\n",
    "The first part of this tutorial is based on material from a tutorial by Dr. Andrea Santamaria Garcia (Karlsruhe Institute of Technology) and Chenran Xu (Karlsruhe Institute of Technology) held at the [Deep Learning School](https://indico.desy.de/event/40559/timetable/) 'Basic Concepts' of the [ErUM-Data-Hub](https://erumdatahub.de).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80499cdf",
   "metadata": {},
   "source": [
    "## Motivation - Why fitting a 1D mathematical function?\n",
    "- Neural networks fitting to a abstract function\n",
    "- To simplify, we use 1d mathematical function to understand the basics of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a1435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imports and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatic execution time tracking\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538eebc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 6, 4\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['image.cmap'] = \"viridis\"\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "plt.rcParams['savefig.bbox'] = \"tight\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5f101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reproducibility\n",
    "\n",
    "- We set the random seeds so that the training results are always the same\n",
    "- Feel free to change the seed number to see the effects of the random initialization of the network weights on the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bcc7b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 26\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.openmp.deterministic = True\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3655e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accelerated computing\n",
    "\n",
    "- **Accelerated computing** = when we add extra hardware to accelerate computation, like GPUs (needed in deep machine learning).\n",
    "- **GPU**: many \"not-so-intelligent\" cores that are parallelizable. They can carry out specific operations in a very efficient way, e.g. tensor cores perform very efficient sparse tensor multiplication.\n",
    "\n",
    "We will be working with torch tensors in this notebook! instead of the usual numpy arrays. This means you could execute this code on a GPU if you have access to one with a simple command `torch.device(\"cuda\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb09c91",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set the GPU device\n",
    "myGPU = 0\n",
    "device = torch.device(f'cuda:{myGPU}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the current device\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(myGPU)\n",
    "\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b239f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conventions for this notebook\n",
    "\n",
    "### Jargon\n",
    "\n",
    "- Unit = activation = neuron\n",
    "- Model = neural network\n",
    "- Feature = dimension of input vector = number of independent variables\n",
    "- Hypothesis = prediction = output of the model\n",
    "\n",
    "### Indices\n",
    "\n",
    "- **Data points:** $i = 1,..., n$ \n",
    "- **Parameters of the model:** $k = 1,..., p$ \n",
    "- **Layers:** $j = 1,..., l$ \n",
    "- **Activation unit label:** $s$ \n",
    "\n",
    "### Scalars\n",
    "\n",
    "- $u^j$ = number of units in layer $j$\n",
    "- $z_s^j$ is the activation unit $s$ in layer $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a9069",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conventions for this notebook\n",
    "\n",
    "### Vectors and matrices\n",
    "\n",
    "- $\\pmb{X}$: input vector of dimension $[n \\times (p \\times 1)]$\n",
    "- $z^j$: activation vector of layer $j$ of dimension $[(u^j + 1) \\times 1]$\n",
    "- $\\pmb{w}^j$: weight matrix from layer $j$ to $j+1$, of dimension $[u^{j+1} \\times (u^j + 1)]$\n",
    "\n",
    "\n",
    "<span style='color:Blue'> where the $+1$ accounts for the bias unit </span>\n",
    "\n",
    "\n",
    "$$\n",
    "\\pmb{X} =\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_p\n",
    "\\end{bmatrix} \\ \\ ; \\ \\\n",
    "\\pmb{w}^j =\n",
    "\\begin{bmatrix}\n",
    "w_{10} & \\dots & w_{1(u^j + 1)}\\\\\n",
    "w_{20} & \\ddots\\\\\n",
    "\\vdots \\\\\n",
    "w_{(u^{j+1}) 0} & & w_{(u^{j+1})(u^j + 1)}\\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c56d4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universal Approximation Theorem (s. [here](https://en.wikipedia.org/wiki/Universal_approximation_theorem))\n",
    "- When the activation function is non-linear, then a two-layer neural network can be proven to be a **universal function approximator**.\n",
    "- <span style='color:red'> This is where the power of neural networks comes from! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df10fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a function to fit\n",
    "Let's create a simple non-linear function to fit with our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27267d5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sample_points = 1e3\n",
    "x_lim = 100\n",
    "x = np.linspace(0, x_lim, int(sample_points))\n",
    "y = np.sin(x * x_lim * 1e-4) * np.cos(x * x_lim * 1e-3) * 3\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.title('Function to be fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457e577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data shape\n",
    "- Our data is 1D, meaning it has only one feature.\n",
    "- We want a model that for a given $x$ it returns the correspondent $y$ value.\n",
    "- This means that a model with one neuron input and a one neuron output suffices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964b9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_input = 1\n",
    "n_out = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c3d99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data reshaping \n",
    "In order for the model to take each point of the data one by one we need to do some additional re-shaping, where we introduce an additional dimension for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f80a6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x_reshape = x.reshape((int(len(x) / n_input), n_input))\n",
    "y_reshape = y.reshape((int(len(y) / n_out), n_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669f0a9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to check the shape change\n",
    "print(x.shape, y.shape)\n",
    "print(x_reshape.shape, y_reshape.shape)\n",
    "print(x[10], x_reshape[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_reshape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ccdbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>PyTorch</h2>\n",
    "\n",
    "<a href=\"https://pytorch.org/\">PyTorch</a> is an optimized tensor library for deep learning using GPUs and CPUS.\n",
    "- A <span style='color:#b51f2a'> **tensor** </span> is an algebraic object that may map between different objects such as vectors, scalars, and even other tensors. It can be easily understood as a multidimensional matrix/array. \n",
    "    - These objects allow to easily carry out machine learning computations in problems with many features, weights, etc.\n",
    "    - In PyTorch, a <a href=\"https://pytorch.org/docs/stable/tensors.html#:~:text=A%20torch.,of%20a%20single%20data%20type.\">tensor</a> is a multi-dimensional matrix containing elements of a single data type.\n",
    "\n",
    "<img src=\"img/tensor_2.jpeg\" style=\"width:40%; margin:auto;\" />\n",
    "<p style=\"clear:both; font-size: small; text-align: center; margin-top:1em;\">image from <a href=\"https://towardsai.net/p/deep-learning/working-with-pytorch-tensors\">Working with PyTorch tensors</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664a613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data type\n",
    "The data that we will input to the model needs to be of the type `torch.float32`\n",
    "\n",
    "_Side Remark_: The default dtype of torch tensors (also the layer parameters) is `torch.float32`, which is related to the GPU performance optimization. If one wants to use `torch.float64`/`torch.double` instead, one can set the tensors to double precision via `v = v.double()` or set the global precision via `torch.set_default_dtype(torch.float64)`. Just keep in mind, the NN parameters and the input tensors should have the same precision.\n",
    "\n",
    "Before starting, let's convert our data numpy arrays to torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy to torch tensor\n",
    "x_torch = torch.from_numpy(x_reshape)\n",
    "y_torch = torch.from_numpy(y_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a39f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type checking:\n",
    "print(x.dtype, y.dtype)\n",
    "print(x_torch.dtype, y_torch.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b269357",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convert data type\n",
    "The type is still not correct, but we can easily convert it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a41119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from float64 to float32\n",
    "x_torch = x_torch.to(dtype=torch.float32)\n",
    "y_torch = y_torch.to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74760bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type checking:\n",
    "print(x.dtype, y.dtype)\n",
    "print(x_torch.dtype, y_torch.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ae34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_torch.numpy(), y_torch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec2b92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data normalization\n",
    "We will also need to normalize the data to make sure we are in the non-linear region of the activation functions:\n",
    "\n",
    "<img src=\"img/activation-functions.png\" width=\"60%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97d515",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate the data normalization\n",
    "We are using min-max normalization to normalize the input tensors to [0,1] and output tensors to [-0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da256a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (x_torch - x_torch.min()) / (x_torch.max() - x_torch.min())\n",
    "y_norm = (y_torch - y_torch.min()) / (y_torch.max() - y_torch.min()) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_norm.detach().numpy(), y_norm.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.title('Normalized function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a959c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build your model\n",
    "- In PyTorch [`Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) stands for *sequential container*, where modules can be added sequentially and are connected in a cascading way. The output for each module is forwarded sequentially to the next.\n",
    "- Now we will build a simple model with one hidden layer with `Sequential`\n",
    "- Remember that every layer in a neural network is followed by an **activation layer** that performs some additional operations on the neurons.\n",
    "\n",
    "<img src=\"./img/sequential.png\" alt=\"drawing\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fc79d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's build 3 different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da2221",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model 0 (1 layer, LeakyReLU)\n",
    "\n",
    "A small model with small non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_01 = 5\n",
    "model0 = nn.Sequential(\n",
    "    nn.Linear(n_input, n_hidden_01),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(n_hidden_01, n_out),\n",
    ").to(device)  # Move model to the correct device\n",
    "print(model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a9b87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model 1 (1 layer, Tanh)\n",
    "\n",
    "A small model with some non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56967956",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_11 = 5\n",
    "model1 = nn.Sequential(nn.Linear(n_input, n_hidden_11),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(n_hidden_11, n_out),\n",
    "                      ).to(device)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36734dbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model 2 (2 layer, optional Tanh or LeakReLU)\n",
    "\n",
    "A larger model with non-linearity\n",
    "Activation function Tanh or optional LeakReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_21 = 10\n",
    "n_hidden_22 = 5\n",
    "# activation function Tanh\n",
    "model2 = nn.Sequential(nn.Linear(n_input, n_hidden_21),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(n_hidden_21, n_hidden_22),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(n_hidden_22, n_out),\n",
    "                      ).to(device)\n",
    "print(model2)\n",
    "\n",
    "# Uncomment to use LeakyReLU instead of Tanh \n",
    "# model2 = nn.Sequential(nn.Linear(n_input, n_hidden_21),\n",
    "#                       nn.LeakyReLU(),\n",
    "#                       nn.Linear(n_hidden_21, n_hidden_22),\n",
    "#                       nn.LeakyReLU(),\n",
    "#                       nn.Linear(n_hidden_22, n_out),\n",
    "#                       )\n",
    "# print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67c104",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#145a32;\">How much do you think each hyperparameter will affect the quality of the model</h3>\n",
    "\n",
    "- <p style=\"color:#145a32;\"> uncomment and execute the next line to explore the methods of the <code>model</code> object you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dad8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding the PyTorch model\n",
    "Try the `parameters` method (needs to be instantiated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59410e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3aa751",
   "metadata": {},
   "source": [
    "The `parameters` method gives back a *generator*, which means it needs to be iterated over to give back an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in model0.parameters():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61096971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:green;\">Without taking into account any bias unit: can you identify the elements of the model by their dimensions?</span>\n",
    "\n",
    "- <span style=\"color:green;\">The first element corresponds to the weight matrix</span> $\\theta^0$ <span style=\"color:green;\">from layer 0 to layer 1, of dimensions</span> $u^{j+1} \\times u^j = u^2 \\times u^1$ <span style=\"color:green;\">(so, without bias)</span>\n",
    "- <span style=\"color:green;\">The second element corresponds to the values of the activation units in layer 1</span>\n",
    "- <span style=\"color:green;\">The third element corresponds to the weight matrix</span> $\\theta^1$ <span style=\"color:green;\">from layer 1 to layer 2, of dimensions</span> $u^{j+1} \\times u^j = u^3 \\times u^3$ <span style=\"color:green;\">(without bias)</span>\n",
    "- <span style=\"color:green;\">The fourth element is the output of the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb37963",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's have a look at what the contents of those tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in model0.parameters():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4fb22",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h3 style=\"color:#145a32;\">What are these values?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07f9f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define the loss function\n",
    "- Reminder: the **loss function** measures how distant the predictions made by the model are from the actual values\n",
    "- `torch.nn` provides many different types of [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions). One of the most popular ones in the [Mean Squared Error (MSE)](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) since it can be applied to a wide variety of cases.\n",
    "- In general cost functions are chosen depending on desirable properties, such as convexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ceaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085beddd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define the optimizer\n",
    "[`torch.optim`](https://pytorch.org/docs/stable/optim.html) provides implementations of various optimization algorithms. The optimizer object will hold the current state and will update the parameters of the model based on computer gradients. It takes as an input an iterable containing the model parameters, that we explored before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85354d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef269d39",
   "metadata": {},
   "source": [
    "### Choose Adam or SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b75147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam-Optimizer\n",
    "optimizer0 = torch.optim.Adam(model0.parameters(), lr=learning_rate)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD-Optimizer\n",
    "# optimizer0 = torch.optim.SGD(model0.parameters(), lr=learning_rate)\n",
    "# optimizer1 = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
    "# optimizer2 = torch.optim.SGD(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ace05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the models on a loop\n",
    "The model learns iteratively in a loop of a given number of epochs. Each loop consists of:\n",
    "- A **forward propagation**: compute $y$ given the input $x$ and current weights and calculate the loss\n",
    "- A **backward propagation**: compute the gradient of the loss function (error of the loss at each unit)\n",
    "- Gradient descent: update model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f444d24",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " Forward propagation             |  Backpropagation\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"img/forwordpropagation.png\" style=\"width:50%; margin:auto;\" /> |<img src=\"img/backpropagation.png\" style=\"width:50%; margin:auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdff9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64 # how many points to pass to the model at a time\n",
    "# batch_size = len(x_norm)  # uncomment to pass all data at once\n",
    "dataset = TensorDataset(x_norm, y_norm)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop NEW\n",
    "def training_loop(dataloader, model, optimizer, epochs):\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            pred_y = model(x_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(pred_y, y_batch)\n",
    "            loss.backward()  # Back-prop\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b23ea",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./img/trainingloop.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455e259",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Run the training for all the models\n",
    "#epochs = 2000\n",
    "epochs = 500\n",
    "\n",
    "losses0 = training_loop(dataloader, model0, optimizer0, epochs=epochs)\n",
    "losses1 = training_loop(dataloader, model1, optimizer1, epochs=epochs)\n",
    "losses2 = training_loop(dataloader, model2, optimizer2, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14805b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "plt.plot(losses0, label='Model 0', color='green')\n",
    "plt.plot(losses1, label='Model 1', color='blue')\n",
    "plt.plot(losses2, label='Model 2', color='red')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(\"Learning rate %f\"%(learning_rate))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a41cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the loss with x-axis epochs (real) and batch-epoch (based on batchsize)\n",
    "# Plot with the primary x-axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(losses0, label='Model 0', color='green')\n",
    "ax1.plot(losses1, label='Model 1', color='blue')\n",
    "ax1.plot(losses2, label='Model 2', color='red')\n",
    "\n",
    "ax1.set_xlabel('Epoch (#Batches)')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title(f\"Learning rate {learning_rate}\")\n",
    "\n",
    "fig.legend()\n",
    "\n",
    "# Adding the secondary x-axis using plt\n",
    "ax2 = ax1.twiny()\n",
    "ax2.set_xlim(0,epochs)  # Synchronize the x-limits\n",
    "ax2.set_xlabel('Epoch (real)')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "print(f'Every epoch has {len(x)/batch_size} (in real {int(len(x)/batch_size)}) batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bac345",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:#145a32;\">Interpreting the loss curves</span>\n",
    "\n",
    "- <span style=\"color:#145a32;\">Have the NNs learned?</span>\n",
    "- <span style=\"color:#145a32;\"> Why is model 0 learning faster than model 1?</span>\n",
    "- <span style=\"color:#145a32;\"> Why is model 2 better than models 0 and 1?</span>\n",
    "- <span style=\"color:#145a32;\"> Train for more epochs. How does the loss curve change?</span>\n",
    "- <span style=\"color:#145a32;\"> Change the number of minibatches to pass all data at once. How does the loss curve change? Which method is more effective?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d5b90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test the trained model\n",
    "- Let's create some random points in the x-axis within the model's interval that will serve as test data.\n",
    "- We will do the same data manipulations as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436395cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_points = 50\n",
    "x_test = np.random.uniform(0, np.max(x_norm.detach().numpy()), test_points)\n",
    "x_test_reshape = x_test.reshape((int(len(x_test) / n_input), n_input))\n",
    "\n",
    "# Convert numpy array to PyTorch tensor and move to the correct device\n",
    "x_test_torch = torch.from_numpy(x_test_reshape).to(dtype=torch.float32).to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a45d3",
   "metadata": {},
   "source": [
    "Now we predict the y-value with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a68ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_test_torch = model0(x_test_torch)\n",
    "y1_test_torch = model1(x_test_torch)\n",
    "y2_test_torch = model2(x_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f53af",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_norm.detach().cpu().numpy(), y_norm.detach().cpu().numpy())\n",
    "plt.scatter(x_test_torch.detach().cpu().numpy(), y0_test_torch.detach().cpu().numpy(), color='green', marker='*', label='Model 0')\n",
    "plt.scatter(x_test_torch.detach().cpu().numpy(), y1_test_torch.detach().cpu().numpy(), color='blue', marker='v', label='Model 1')\n",
    "plt.scatter(x_test_torch.detach().cpu().numpy(), y2_test_torch.detach().cpu().numpy(), color='red', label='Model 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703024a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#145a32;\">Comment on the NN predictions</h3>\n",
    "\n",
    "- <span style=\"color:#145a32;\"> Why does the prediction of model 0 have that particular shape?</span>\n",
    "- <span style=\"color:#145a32;\"> Which activation function would be more appropriate to fit this function, the one from model 0 or model 1?</span>\n",
    "- <span style=\"color:#145a32;\"> Which NN gets the best prediction and why?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c4eb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:#145a32;\">Bonus</h3>\n",
    "\n",
    "- <span style=\"color:#145a32;\">$\\implies$ Change the seed at the top of the notebook. How do the predictions change?</span>\n",
    "- <span style=\"color:#145a32;\">$\\implies$ Change the optimizer in <code>Section 4</code> from <code>Adam</code> to <code>SGD</code> and re-train the models. What happens? How did the loss curves change? Did the NNs learn? Change the number of epochs and try to make it learn.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca70bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Play with the notebook!\n",
    "Some ideas:\n",
    "- Change the number of epochs in `Section 5` to 5000 and re-train the models. What happens?\n",
    "- Change the random seed in the `Reproducibility` cell at the very top. How do the results change?\n",
    "- Change the optimizer in `Section 4` from `Adam` to `SGD` and re-train the models. What happens?\n",
    "- [**if time allows, takes several minutes**] Change the epochs in `Section 5` to 1000000. What happens?\n",
    "- Go back to 1000 epochs and the Adam optimizer. Change the learning rate in `Section 4` to 0.05. How do the results change? what does it tell us about our previous value?\n",
    "- Change the learning rate to 0.5. What happens now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27524e1a",
   "metadata": {},
   "source": [
    "# Physics-informed neural networks (PINNs)\n",
    "\n",
    "This is a deep learning model that incorporates **physical laws and constraints directly into its architecture through the loss function**. PINNs solve differential equations by training neural networks to satisfy both the governing equations and boundary/initial conditions simultaneously.\n",
    "\n",
    "PINNs are valuable because they:\n",
    "1. Can handle complex, high-dimensional problems where analytical solutions are intractable\n",
    "2. Provide continuous, differentiable solutions rather than discrete numerical approximations\n",
    "3. Can solve both forward and inverse problems within the same framework\n",
    "\n",
    "<img src=\"img/PINN.png\" alt=\"PINN\" style=\"width:850px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c9e65",
   "metadata": {},
   "source": [
    "## Damped Simple Harmonic Oscillator PINN\n",
    "\n",
    "As an example of a PINN we solve the damped oscillator ordinary differential equation\n",
    "\n",
    "$$ u''(t) + \\gamma u'(t) + u(t) = 0, $$\n",
    "\n",
    "with initial conditions\n",
    "\n",
    "$$ u(0)=1, \\quad u'(0)=0. $$\n",
    "\n",
    "The exact (underdamped) solution is given by\n",
    "\n",
    "$$ u(t)=e^{-\\gamma t}\\Big(\\cos(\\omega_d t) + \\frac{\\gamma}{\\omega_d}\\sin(\\omega_d t)\\Big), \\quad \\text{with}\\quad \\omega_d=\\sqrt{1-\\gamma^2}. $$\n",
    "\n",
    "We will define a neural network that approximates \\( u(t) \\), enforce the governing differential equation and initial conditions through tailored loss functions, and explore the training process with interactive widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ed9d8",
   "metadata": {},
   "source": [
    "#### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9a9e8",
   "metadata": {},
   "source": [
    "## Define the PINN Model\n",
    "\n",
    "We now define the neural network that represents our PINN. This network is a fully connected network with two hidden layers (each with 20 neurons and a $\\tanh $activation function) that approximates $u(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 20),  # Input: time t, Output: 20 neurons\n",
    "            nn.Tanh(),         # Tanh activation function\n",
    "            nn.Linear(20, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 1)   # Output: u(t)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Instantiate and display the model structure\n",
    "model = PINN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696f04e",
   "metadata": {},
   "source": [
    "## Physics-Informed Loss Functions\n",
    "\n",
    "To train our PINN, we enforce the governing differential equation and the initial conditions using specialized loss functions:\n",
    "\n",
    "1. **PDE Loss:** Computes the residual of the differential equation\n",
    "   $$ u''(t) + \\gamma u'(t) + u(t) $$\n",
    "   using PyTorch's automatic differentiation via `torch.autograd.grad`.\n",
    "\n",
    "2. **Initial Condition Loss:** Enforces the conditions $$u(0)=1, \\quad u'(0)=0.$$ \n",
    "\n",
    "An additional **Data Loss** is also used by sampling a few known solution points from the exact solution to guide training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_loss(model, t, gamma):\n",
    "    # Ensure input t requires gradient\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Compute the network output u(t)\n",
    "    u = model(t)\n",
    "    \n",
    "    # First derivative u'(t)\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    \n",
    "    # Second derivative u''(t)\n",
    "    u_tt = torch.autograd.grad(u_t, t, grad_outputs=torch.ones_like(u_t), create_graph=True)[0]\n",
    "    \n",
    "    # Residual of the ODE: u''(t) + gamma*u'(t) + u(t) = 0\n",
    "    f = u_tt + gamma * u_t + u\n",
    "    loss_f = torch.mean(f**2)\n",
    "    return loss_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f232d",
   "metadata": {},
   "source": [
    "### Initial Condition Loss Function\n",
    "\n",
    "This function enforces the initial conditions by calculating the discrepancy between the network prediction and the true values at \\(t=0\\). It also computes the derivative \\(u'(0)\\) and penalizes deviations from the prescribed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc27df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_loss(model, t0, u0_true, u0_t_true):\n",
    "    # Set requires_grad for the initial condition point\n",
    "    t0 = t0.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Compute u(t0) and enforce u(t0) = u0_true\n",
    "    u0 = model(t0)\n",
    "    loss_u0 = (u0 - u0_true)**2\n",
    "    \n",
    "    # Compute the first derivative u'(t0) and enforce u'(t0) = u0_t_true\n",
    "    u0_t = torch.autograd.grad(u0, t0, grad_outputs=torch.ones_like(u0), create_graph=True)[0]\n",
    "    loss_u0_t = (u0_t - u0_t_true)**2\n",
    "    \n",
    "    return torch.mean(loss_u0) + torch.mean(loss_u0_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa889370",
   "metadata": {},
   "source": [
    "## Define the Exact Solution\n",
    "\n",
    "For validation purposes, we define the exact solution given by\n",
    "\n",
    "$$ u(t)=e^{-\\gamma t}\\Big(\\cos(\\omega_d t) + \\frac{\\gamma}{\\omega_d}\\sin(\\omega_d t)\\Big), \\quad \\omega_d=\\sqrt{1-\\gamma^2}. $$\n",
    "\n",
    "This function will be used to generate training data as well as to compare against the PINN prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(t, gamma, omega_d):\n",
    "    return np.exp(-gamma * t) * (np.cos(omega_d * t) + (gamma / omega_d) * np.sin(omega_d * t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f951fe1",
   "metadata": {},
   "source": [
    "## Training Function with Interactive Widget\n",
    "\n",
    "The following function performs the training of the PINN. It:\n",
    "\n",
    "- Generates training data by sampling points from the exact solution on the interval $[0, t_{train\\_end}]$.\n",
    "- Creates collocation points over the broader interval $[0, 10]$ to enforce the differential equation.\n",
    "- Minimizes a total loss composed of the PDE loss, the initial condition loss, and the data loss.\n",
    "- Plots the network prediction every 500 epochs alongside the exact solution and training data.\n",
    "\n",
    "The function parameters (number of epochs, learning rate, $\\gamma$, and the end of training data) can be adjusted interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pinn(epochs=5000, learning_rate=0.001, gamma=0.12, t_train_end=4.0):\n",
    "    # Time parameters\n",
    "    t_train_start = 0.0  # Start of training data\n",
    "    t_test_end = 10.0    # End of prediction range\n",
    "\n",
    "    # Define the model and optimizer\n",
    "    model = PINN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Generate training data (10 points from the exact solution within [0, t_train_end])\n",
    "    num_train_points = 10\n",
    "    t_train = np.random.uniform(t_train_start, t_train_end, num_train_points)[:, None]\n",
    "    omega_d = np.sqrt(1 - gamma**2)\n",
    "    u_train = exact_solution(t_train, gamma, omega_d)\n",
    "    \n",
    "    # Convert training data to tensors\n",
    "    t_train_tensor = torch.tensor(t_train, dtype=torch.float32, requires_grad=True)\n",
    "    u_train_tensor = torch.tensor(u_train, dtype=torch.float32)\n",
    "    \n",
    "    # Initial condition at t = 0\n",
    "    t0 = torch.tensor([[0.0]], dtype=torch.float32, requires_grad=True)\n",
    "    u0_true = torch.tensor([[1.0]], dtype=torch.float32)  # u(0) = 1\n",
    "    u0_t_true = torch.tensor([[0.0]], dtype=torch.float32)  # u'(0) = 0\n",
    "    \n",
    "    # Generate collocation points for the entire range [0, 10]\n",
    "    num_colloc_points = 100\n",
    "    t_colloc = np.random.uniform(t_train_start, t_test_end, num_colloc_points)[:, None]\n",
    "    t_colloc_tensor = torch.tensor(t_colloc, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Training loop\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the PDE loss\n",
    "        loss_pde = pde_loss(model, t_colloc_tensor, gamma)\n",
    "        \n",
    "        # Compute the initial condition loss\n",
    "        loss_ic = ic_loss(model, t0, u0_true, u0_t_true)\n",
    "        \n",
    "        # Compute data loss from training data\n",
    "        loss_data = torch.mean((model(t_train_tensor) - u_train_tensor)**2)\n",
    "        \n",
    "        total_loss = loss_pde + loss_ic + loss_data\n",
    "        loss_history.append(total_loss.item())\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Plot every 500 epochs\n",
    "        if epoch % 500 == 0:\n",
    "            t_test = np.linspace(t_train_start, t_test_end, 200)[:, None]\n",
    "            t_test_tensor = torch.tensor(t_test, dtype=torch.float32)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                u_pred = model(t_test_tensor).cpu().numpy()\n",
    "            \n",
    "            u_exact = exact_solution(t_test, gamma, omega_d)\n",
    "            \n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(t_test, u_exact, color='gray', linestyle='-', label='Exact solution', linewidth=2)\n",
    "            plt.plot(t_test, u_pred, 'b-', label='Neural network prediction', linewidth=2)\n",
    "            plt.scatter(t_train, u_train, color='orange', label='Training data', zorder=5)\n",
    "            plt.axvline(x=t_train_end, color='r', linestyle='--', label='End of training data')\n",
    "            plt.title(f'Training step: {epoch} | Training data ends at t = {t_train_end}')\n",
    "            plt.xlabel('t')\n",
    "            plt.ylabel('u(t)')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    \n",
    "    # Plot the loss history\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.semilogy(loss_history, 'k-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8110b9",
   "metadata": {},
   "source": [
    "## Interactive Widget for Training\n",
    "\n",
    "The interactive widget below lets you adjust the training parameters:\n",
    "\n",
    "- **epochs:** The number of training iterations.\n",
    "- **Learning rate:** The step size for the optimizer.\n",
    "- **gamma, $\\gamma$:** The damping factor in the ODE.\n",
    "- **Train end:** The end time for generating training data.\n",
    "\n",
    "By adjusting these sliders, you can observe how the network prediction evolves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b83e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    train_pinn,\n",
    "    epochs=IntSlider(min=1000, max=10000, step=1000, value=5000),\n",
    "    learning_rate=FloatSlider(min=0.0001, max=0.01, step=0.0001, value=0.001, readout_format='.4f', description='Learning rate'),\n",
    "    gamma=FloatSlider(min=0.01, max=0.5, step=0.01, value=0.12),\n",
    "    t_train_end=FloatSlider(min=1.0, max=8.0, step=0.5, value=4.0, description='Train end')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5aa5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outlook\n",
    "5 min break\n",
    "#### Hands-On Session 2 -- Classification and More\n",
    "[DPG Regensburg](https://www.dpg-verhandlungen.de/year/2025/conference/regensburg/part/tut/session/1)\n",
    "##### Abstract: \n",
    "The session demonstrates how pre-trained models can simplify tasks such as classification, making them readily applicable to research. Typical examples include recognizing handwritten digits, which showcase the power of pretrained models in solving common challenges. As a preview of advanced topics, the tutorial concludes with brief examples of large language models (LLMs) and generative AI.\n",
    "\n",
    "Keywords: AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06ddde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[ErUM-Data-Hub<img src=\"./img/erum-data-hub_logo.png\" style=\"width: 20%;\" />](https://erumdatahub.de)\n",
    "\n",
    "#### Learn more about us, our events and shools Do, AKPIK 5.10:\n",
    "[AKPIK 5.10: Poster Donnerstag, 20. März 2025, 15:00–16:30, P2](https://www.dpg-verhandlungen.de/year/2025/conference/regensburg/part/akpik/session/5/contribution/10) Advancing Digital Transformation in Research on Universe and Matter in Germany\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "venv-conf",
   "language": "python",
   "name": "venv-conf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

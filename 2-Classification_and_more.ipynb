{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial: AI Fundamentals for Research (joint session BP/TUT/DY/AKPIK)\n",
    "Sunday, 16th March 2025, 16:00–18:15, H2<br>\n",
    "Presented at the **[2025 DPG Spring Meeting of the Condensed Matter Section](https://www.dpg-verhandlungen.de/year/2025/conference/regensburg/part/tut/session/1)**\n",
    "\n",
    "## Hands-On Session 2: Classification and More\n",
    "\n",
    "#### Jan Bürger<sup>1</sup>, Janine Graser<sup>2</sup>, •Robin Msiska<sup>2,3</sup>, and Arash Rahimi-Iman<sup>4</sup>\n",
    "\n",
    "<sup>1</sup>ErUM-Data-Hub, RWTH Aachen University, Aachen, Germany<br>\n",
    "<sup>2</sup>Faculty of Physics and Center for Nanointegration Duisburg-Essen (CENIDE), University of Duisburg-Essen, Duisburg, Germany<br>\n",
    "<sup>3</sup>Department of Solid State Sciences, Ghent University, Ghent, Belgium<br>\n",
    "<sup>4</sup>I. Physikalisches Institut and Center for Materials Research, Justus-Liebig-University Gießen, Gießen, Germany\n",
    "\n",
    "### Abstract\n",
    "*The session demonstrates how pre-trained models can simplify tasks such as classification, making them readily applicable to research. Typical examples include recognizing handwritten digits, which showcase the power of pretrained models in solving common challenges. As a preview of advanced topics, the tutorial concludes with brief examples of large language models (LLMs) and generative AI.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### About this notebook\n",
    "#### **Presenter: Robin Msiska**\n",
    "\n",
    "In this notebook, we'll explore three fundamental applications of neural networks in image processing:\n",
    "\n",
    "1. **Image Classification**: Identifying what's in an image (e.g., recognising handwritten digits\n",
    "\n",
    "2. **Image Denoising**: Cleaning up noisy or corrupted images\n",
    "3. **Transfer Learning**: Using knowledge from one task to improve performance on another\n",
    "\n",
    "We'll use the MNIST (Modified National Institute of Standards and Technology) dataset of handwritten digits as our playground. While simple, this dataset allows us to understand core concepts that scale to more complex research-specific tasks.\n",
    "\n",
    "<img src=\"img/mnist-digits-classification.png\" alt=\"MNIST\" width=\"800\"/>\n",
    "\n",
    "### Why These Topics Matter\n",
    "\n",
    "These techniques form the foundation of many real-world AI applications. Here are some ways that they are used in condensed matter physics:\n",
    "\n",
    "**Classification**\n",
    "- Detecting anomalies in experimental measurements <br>\n",
    "e.g. P. Napoletano, F. Piccoli, & R. Schettini, [*Sensors* **18**(1), 209 (2018)](https://www.mdpi.com/1424-8220/18/1/209)<br>\n",
    "<img src=\"img/eg_anomaly.png\" alt=\"Paper CNN anomaly\" width=\"400\"/>\n",
    "\n",
    "- Classifying phase transitions in material systems<br>\n",
    "e.g. K. Kottmann, P. Huembeli, M. Lewenstein, and A. Acín, [*Phys. Rev. Lett.* **125**, 170603 (2020)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.170603)<br>\n",
    "<img src=\"img/eg_phase.png\" alt=\"Paper CNN phase transition\" width=\"400\"/>\n",
    "\n",
    "**Denoising**\n",
    "- Enhancing microscopy and imaging data\n",
    "- Cleaning experimental signals from background noise<br>\n",
    "e.g. K. Wu et al., [*Anal. Chem.* **93**, 3, 1377–1382 (2021)](https://pubs.acs.org/doi/abs/10.1021/acs.analchem.0c03087)<br>\n",
    "<img src=\"img/eg_denoise.png\" alt=\"Paper denoising\" width=\"400\"/>\n",
    "\n",
    "**Transfer Learning**\n",
    "- Applying models trained on simulated physics data to real experiments\n",
    "- Adapting solutions between similar physical systems\n",
    "- Leveraging knowledge between different energy scales<br>\n",
    "e.g. D. Jha, K. Choudhary, F. Tavazza et al, [*Nat Commun* **10**, 5316 (2019)](https://www.nature.com/articles/s41467-019-13297-w)<br>\n",
    "<img src=\"img/eg_transfer.png\" alt=\"Paper transfer learning\" width=\"300\"/>\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How Convolutional Neural Networks (CNNs) work for image classification\n",
    "- How autoencoders can clean up noisy images\n",
    "- How to transfer knowledge from one trained neural network to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the GPU device\n",
    "myGPU = 0\n",
    "device = torch.device(f'cuda:{myGPU}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set the current device\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(myGPU)\n",
    "\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatic execution time tracking\n",
    "# run this cell only, if you are allowed to write on the disk and install packages\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Part 1: Image Classification with CNNs\n",
    "\n",
    "### What is Image Classification?\n",
    "\n",
    "Image classification is the task of assigning a label to an image from a fixed set of categories.\n",
    "\n",
    "MNIST is a classic dataset of handwritten digits. It contains:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- Each image is 28×28 pixels in grayscale (0-255 pixel values)\n",
    "- 10 classes (digits 0-9)\n",
    "\n",
    "Below we load the MNIST dataset using PyTorch utilities and visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Convert the data and labels to NumPy arrays\n",
    "X = np.array(mnist.data, dtype=np.float32)  # shape: (70000, 784)\n",
    "y = np.array(mnist.target, dtype=np.int64)    # shape: (70000,)\n",
    "\n",
    "# Normalize pixel values and reshape to (N, 1, 28, 28)\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, 28, 28)\n",
    "X = torch.tensor(X).unsqueeze(1)  # add channel dimension\n",
    "\n",
    "# Split into training and test sets (60,000 for training and 10,000 for testing)\n",
    "x_train, y_train = X[:60000], torch.tensor(y[:60000])\n",
    "x_test, y_test = X[60000:], torch.tensor(y[60000:])\n",
    "\n",
    "print('Training set shape:', x_train.shape)\n",
    "print('Test set shape:', x_test.shape)\n",
    "\n",
    "# Visualize 10 examples\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    # squeeze channel dimension for visualization\n",
    "    plt.imshow(x_train[i].squeeze(0), cmap='gray')\n",
    "    plt.title(f'Label: {y_train[i].item()}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('MNIST Dataset Examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Convolutional Neural Networks (CNNs)?\n",
    "CNNs are a specialized type of neural network designed to process grid-like data such as images.\n",
    "#### A Visual Overview of our CNN\n",
    "<img src=\"img/CNN.png\" alt=\"CNN\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layer\n",
    "\n",
    "This is Made up of:\n",
    "- **Convolutional Layers** that slide over the image to extract local features.\n",
    "\n",
    "- **Activation Functions** that introduce non-linearity; for example, ReLU $ f(x) = \\max(0, x) $\n",
    "\n",
    "<img src=\"img/CNN_cov.png\" alt=\"Convolution operation\" width=\"600\"/>\n",
    "\n",
    "##### Convolutional operation\n",
    "The convolution operation is at the core of CNNs.  The convolution operation is mathematically described as:\n",
    "\n",
    "$$ (F * I)_{x,y} = \\sum_{i}\\sum_{j} F_{i,j} \\cdot I_{x+i,y+j} $$\n",
    "\n",
    "<img src=\"img/conv_op.png\" alt=\"Convolution operation\" width=\"800\"/> ~ B.K. Kalejahi, S. Meshgini, S. Danishvar, S. Khorram, [Intelligent Data Analysis 26(4), 1097-1114 (2022)](https://journals.sagepub.com/doi/full/10.3233/IDA-216379)\n",
    "\n",
    "The small matrix of weights in the image is called the kernel or filter and it slides across an image, applying the same pattern-detecting weights at each position. This reuse of weights (parameter sharing) helps detect features regardless of their location in the image.\n",
    "\n",
    "![same_padding_no_strides.gif](img/same_padding_no_strides.gif)\n",
    "~ animation from D. Vincent, F. Visin. [arXiv:1603.07285 (2016)](https://arxiv.org/abs/1603.07285)\n",
    "##### DIfferent kernels lead to different image processing results\n",
    "\n",
    "For instance with kernel \n",
    "$\\begin{bmatrix}\n",
    "1/9 & 1/9 & 1/9 \\\\\n",
    "1/9 & 1/9 & 1/9 \\\\\n",
    "1/9 & 1/9 & 1/9\n",
    "\\end{bmatrix}$ we can blur an image\n",
    "\n",
    "<img src=\"img/gaussian_blur_demo_400x400_fixed.gif\" alt=\"Gaussian\" width=\"800\"/>\n",
    "\n",
    "A different kernel\n",
    "$\\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -2 & -1\n",
    "\\end{bmatrix}$ would yield:\n",
    "\n",
    "<img src=\"img/sobel.png\" alt=\"Sobel\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling\n",
    "\n",
    "<img src=\"img/CNN_maxpool.png\" alt=\"CNN maxpool\" width=\"600\"/>\n",
    "\n",
    "In CNN architectures, it is typical that the spatial dimension of the data is reduced periodically via pooling layers. Pooling layers are typically used after a series of convolutional layers to reduce the spatial size of the activation maps. You can think of this as a way to summarise the features from an activation map. Using a pooling layer will reduce the number of parameters in the network because the input size to subsequent layers is reduced. This is a desirable effect because the computations required for training are also reduced. Also, using fewer parameters often helps to mitigate the effects of overfitting. Here we use **Max pooling**.\n",
    "\n",
    "<img src=\"img/MaxPool.png\" alt=\"Max Pooling\" width=\"800\"/>\n",
    "\n",
    "- **Pooling Layers (MaxPooling):** Downsample the image by reducing its spatial dimensions. With a 2×2 window, the pooling operation is:\n",
    "\n",
    "  $$ \\mathrm{MaxPool}(X)_{i,j} = \\max(X_{2i,2j}, X_{2i,2j+1}, X_{2i+1,2j}, X_{2i+1,2j+1}) $$\n",
    "\n",
    "#### Dense Layers\n",
    "Fully connected layers that combine features to perform classification.\n",
    "\n",
    "<img src=\"img/CNN_dense.png\" alt=\"CNN dense\" width=\"600\"/>\n",
    "\n",
    "Softmax is used in the final layer of the network to transform the network's output (which is a vector of raw logits) into probabilities. Softmax takes converts output into a probability distribution by applying the following formula to each element $ z_i $ of the output vector:\n",
    "\n",
    "  $$ P(y_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ P(y_i) $ is the probability of class $ i $\n",
    "- $ z_i $ is the corresponding to class $ i $\n",
    "- The denominator is the sum over all classes.\n",
    "\n",
    "The output of Softmax is a vector where each value represents the probability that the input image belongs to a particular class. The class with the highest probability is the predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "\n",
    "- **Convolutional layers**: Apply sliding filters across the image to detect features like edges, textures, and patterns\n",
    "- **Pooling layers**: Reduce the spatial dimensions of the data, making the network more computationally efficient and robust to variations\n",
    "- **Local connectivity**: Each neuron connects to only a small region of the input, unlike fully-connected networks\n",
    "- **Parameter sharing**: The same weights are applied across the entire image\n",
    "\n",
    "These properties make CNNs extremely efficient for image processing tasks.\n",
    "\n",
    "We will now implement a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten the 2D feature maps for the fully connected layers\n",
    "        # Calculate the size of the flattened features\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected (dense) layers\n",
    "        # Calculate the size of the flattened features after the second pooling layer\n",
    "        # After two poolings, image size reduces from 28x28 to 7x7\n",
    "        #self.fc1 = nn.Linear(64 * 7 * 7, 64)\n",
    "        self.fc1 = nn.Linear(64 * (input_shape[1] // 4) * (input_shape[2] // 4), 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten the 2D feature maps\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply softmax to the output layer\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_shape = (1, 28, 28)  # Example input shape for MNIST dataset (1 channel, 28x28 images)\n",
    "classification_model = CNNModel(input_shape)\n",
    "\n",
    "# Print the model architecture\n",
    "print(classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "We will train the classification model using the following settings:\n",
    "\n",
    "- **Epochs**: 5\n",
    "- **Batch Size**: 128\n",
    "- **Validation Split**: 10% of the training data\n",
    "\n",
    "Below is a training loop using PyTorch that tracks training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Split training set into train and validation subsets (90/10 split)\n",
    "num_train = len(x_train)\n",
    "split = int(0.9 * num_train)\n",
    "\n",
    "train_imgs, val_imgs = x_train[:split], x_train[split:]\n",
    "train_labels, val_labels = y_train[:split], y_train[split:]\n",
    "\n",
    "train_ds = TensorDataset(train_imgs, train_labels)\n",
    "val_ds   = TensorDataset(val_imgs, val_labels)\n",
    " \n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classification_model = classification_model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    train_loss_history.append(epoch_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    \n",
    "    # Validation\n",
    "    classification_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = classification_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / total_val\n",
    "    val_acc = correct_val / total_val\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the training history\n",
    "\n",
    "We visualize the training and validation accuracy and loss over epochs during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_history, label='Training Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classification Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Classification Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Add vertical lines and format x-axis to show only integer epochs\n",
    "for ax in plt.gcf().axes:  # Loop through all subplots\n",
    "    ax.set_xticks(np.arange(0, len(train_acc_history), 1))  # Set x-ticks to integer epochs\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)  # Add faint vertical grid lines\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model test Set\n",
    "We now evaluate the trained classification model on the test set, computing the test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classification model on the test set\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "classification_model.eval()\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= total_test\n",
    "test_acc = correct_test / total_test\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model prediction\n",
    "We now visualize the predictions of a trained classification model on 8 randomly selected test samples. Output shows the images along with the predicted and actual labels, highlighting any incorrect predictions in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model predictions on 8 random test samples\n",
    "indices = np.random.choice(x_test.shape[0], 8, replace=False)\n",
    "sample_images = x_test[indices]\n",
    "sample_labels = y_test[indices]\n",
    "\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(sample_images.to(device))\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(sample_images[i].squeeze(0), cmap='gray')\n",
    "    title_color = 'black'\n",
    "    if preds[i] != sample_labels[i].item():\n",
    "        title_color = 'red'\n",
    "    plt.title(f'Pred: {preds[i]}\\nActual: {sample_labels[i].item()}', color=title_color)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Model Predictions on Test Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "A confusion matrix is a tool used to evaluate the performance of a classification model. It shows the actual vs. predicted classifications in a table format, helping you to understand how well your model is performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtain predictions on the complete test set\n",
    "all_preds = []\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = classification_model(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "cm = confusion_matrix(y_test.numpy(), all_preds)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test.numpy(), all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Transfer Learning for Image Segmentation\n",
    "Image segmentation is the process of partitioning an image into multiple segments or regions, often to simplify or change the representation of an image. The goal is to assign a label to each pixel in the image, indicating whether it belongs to a specific object, background, or region of interest.\n",
    "\n",
    "<img src=\"img/dog1.jpg\" alt=\"Autoencorder\" width=\"400\"/><img src=\"img/dog2.png\" alt=\"Autoencorder\" width=\"400\"/>\n",
    "\n",
    "In this section we build an encoder–decoder model (a common architecture for segmentation) in PyTorch.\n",
    "\n",
    "<img src=\"img/AE.png\" alt=\"Autoencorder\" width=\"800\"/>\n",
    "\n",
    "Phase 1: We pre-train the model to segment MNIST digits (background=0, digit=1).\n",
    "\n",
    "Phase 2: We augment the images by adding randomly rotated white hearts and update the masks accordingly (hearts are labeled as 2), then fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # Output layer (for segmentation, we output raw logits)\n",
    "        self.out_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.dec1(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "# For segmentation, we create masks from the MNIST images\n",
    "# Pixels > 0.1 are considered part of the digit (class 1); background = 0\n",
    "y_train_digits = (x_train > 0.1).squeeze(1).long()  # shape: [60000, 28, 28]\n",
    "y_test_digits  = (x_test > 0.1).squeeze(1).long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-train segmentation: Phase 1 (2 classes: background and digit)\n",
    "We now train a segmentation model (`SegmentationModel`)  to differentiate between the background and digits in an image. This model can later be fine-tuned for more complex tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full training set for segmentation\n",
    "num_seg = x_train.size(0)\n",
    "split_seg = int(0.9 * num_seg)\n",
    "\n",
    "seg_train_imgs = x_train[:split_seg]\n",
    "seg_train_masks = y_train_digits[:split_seg]\n",
    "seg_val_imgs   = x_train[split_seg:]\n",
    "seg_val_masks  = y_train_digits[split_seg:]\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "seg_train_ds = TensorDataset(seg_train_imgs, seg_train_masks)\n",
    "seg_val_ds   = TensorDataset(seg_val_imgs, seg_val_masks)\n",
    "\n",
    "seg_train_loader = DataLoader(seg_train_ds, batch_size=128, shuffle=True)\n",
    "seg_val_loader   = DataLoader(seg_val_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "model_phase1 = SegmentationModel(num_classes=2).to(device)\n",
    "criterion_seg = nn.CrossEntropyLoss()\n",
    "optimizer_seg = optim.Adam(model_phase1.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Phase 1: Segmenting digits (background vs. digit)...\")\n",
    "num_epochs_seg = 5\n",
    "for epoch in range(num_epochs_seg):\n",
    "    model_phase1.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, masks in seg_train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer_seg.zero_grad()\n",
    "        outputs = model_phase1(imgs)  # shape: [B, 2, 28, 28]\n",
    "        loss = criterion_seg(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer_seg.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running_loss / len(seg_train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_seg}] - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the pretrained weights for transfer learning\n",
    "torch.save(model_phase1.state_dict(), 'pretrained_model_phase1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colormap: Light Blue for background and Green for digit\n",
    "custom_cmap = ListedColormap(['lightblue', 'green'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_phase1.eval()\n",
    "\n",
    "# Function to visualize images and masks with custom color scheme\n",
    "def visualize_results(model, dataloader, device, num_images=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of validation data\n",
    "        for imgs, masks in dataloader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            # Run the model on the input batch\n",
    "            outputs = model(imgs)\n",
    "            # Get the predicted segmentation masks (taking argmax to get the class)\n",
    "            pred_masks = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Convert tensor to numpy for plotting\n",
    "            imgs = imgs.cpu().numpy()\n",
    "            masks = masks.cpu().numpy()\n",
    "            pred_masks = pred_masks.cpu().numpy()\n",
    "\n",
    "            # Plot the results\n",
    "            fig, axes = plt.subplots(num_images, 3, figsize=(12, 15))\n",
    "            for i in range(num_images):\n",
    "                # Original Image\n",
    "                axes[i, 0].imshow(imgs[i][0], cmap='gray')\n",
    "                axes[i, 0].set_title(\"Original Image\")\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                # True Mask with custom color map\n",
    "                axes[i, 1].imshow(masks[i], cmap=custom_cmap)\n",
    "                axes[i, 1].set_title(\"True Mask\")\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                # Predicted Mask with custom color map\n",
    "                axes[i, 2].imshow(pred_masks[i], cmap=custom_cmap)\n",
    "                axes[i, 2].set_title(\"Predicted Mask\")\n",
    "                axes[i, 2].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "            break  # Stop after visualizing the first batch\n",
    "\n",
    "# Visualize the results on a few images from the validation set\n",
    "visualize_results(model_phase1, seg_val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Task: Adding Hearts to the Images\n",
    "\n",
    "Now we add randomly rotated white hearts to the MNIST images and update the segmentation masks by assigning the heart area the class label 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import polygon\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def add_random_heart(image, mask):\n",
    "    \"\"\"\n",
    "    Add a randomly rotated white heart with slightly softened edges to the image and update the mask\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    msk = mask.copy()\n",
    "    \n",
    "    # Ensure correct dimensionality\n",
    "    img2d = img[0] if (img.ndim == 3 and img.shape[0] == 1) else img\n",
    "    H, W = img2d.shape\n",
    "\n",
    "    heart_size = random.randint(5, 10)\n",
    "    t = np.linspace(0, 2 * np.pi, 100)\n",
    "    \n",
    "    # Create a heart shape using a parametric equation\n",
    "    x = heart_size * 16 * np.sin(t)**3\n",
    "    y = -heart_size * (13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t))\n",
    "    \n",
    "    # Normalize and center the heart\n",
    "    x = (x - x.min()) / (x.max() - x.min()) * (heart_size * 1.1)\n",
    "    y = (y - y.min()) / (y.max() - y.min()) * (heart_size * 1.1)\n",
    "    x, y = x - x.mean(), y - y.mean()\n",
    "    pts = np.vstack((x, y)).T\n",
    "    \n",
    "    # Apply a random rotation\n",
    "    angle = random.uniform(0, 360)\n",
    "    rad = np.deg2rad(angle)\n",
    "    R = np.array([[np.cos(rad), -np.sin(rad)], [np.sin(rad), np.cos(rad)]])\n",
    "    pts_rot = pts.dot(R.T)\n",
    "    \n",
    "    # Shift the heart to a random valid position in the image\n",
    "    min_xy = pts_rot.min(axis=0)\n",
    "    box_wh = np.ceil(pts_rot.max(axis=0) - min_xy).astype(int)\n",
    "    offset_x = random.randint(0, W - box_wh[0] - 1) if (W - box_wh[0] - 1) > 0 else 0\n",
    "    offset_y = random.randint(0, H - box_wh[1] - 1) if (H - box_wh[1] - 1) > 0 else 0\n",
    "    pts_final = pts_rot - min_xy + np.array([offset_x, offset_y])\n",
    "    \n",
    "    # Create a binary heart mask using polygon filling\n",
    "    heart_binary = np.zeros((H, W), dtype=float)\n",
    "    rr, cc = polygon(pts_final[:, 1], pts_final[:, 0], shape=(H, W))\n",
    "    heart_binary[rr, cc] = 1.0\n",
    "\n",
    "    # Apply a slight Gaussian blur to soften the heart's edge\n",
    "    sigma = 0.5\n",
    "    heart_blurred = gaussian_filter(heart_binary, sigma=sigma)\n",
    "    \n",
    "    # Blend the heart into the original image (the heart is white = 1.0)\n",
    "    img2d = (1 - heart_blurred) * img2d + heart_blurred * 1.0\n",
    "    \n",
    "    # Update the segmentation mask with the original, sharp heart polygon\n",
    "    msk[rr, cc] = 2\n",
    "\n",
    "    # Restore the channel dimension if needed\n",
    "    if img.ndim == 3 and img.shape[0] == 1:\n",
    "        img = np.expand_dims(img2d, axis=0)\n",
    "    else:\n",
    "        img = img2d\n",
    "        \n",
    "    return img, msk\n",
    "\n",
    "# Helper function just for visualizing the changes to the original image\n",
    "def overlay_red(image, mask):\n",
    "    \"\"\"\n",
    "    Convert a grayscale image into an RGB image and overlay the heart region (mask==2) in red\n",
    "    \"\"\"\n",
    "    # Convert grayscale to RGB\n",
    "    if image.ndim == 2:\n",
    "        rgb = np.stack([image, image, image], axis=-1)\n",
    "    elif image.ndim == 3 and image.shape[0] == 1:\n",
    "        rgb = np.stack([image[0]] * 3, axis=-1)\n",
    "    else:\n",
    "        rgb = image.copy()\n",
    "    \n",
    "    # Replace the heart region with red (assuming image values are normalized to [0, 1])\n",
    "    red_color = np.array([1.0, 0.0, 0.0])\n",
    "    rgb[mask == 2] = red_color\n",
    "    return rgb\n",
    "\n",
    "def augment_dataset(x, y, subset_size):\n",
    "    \"\"\"\n",
    "    Select a subset of examples, apply heart augmentation, and return the augmented images and masks\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays if necessary\n",
    "    try:\n",
    "        x_np, y_np = x.cpu().numpy(), y.cpu().numpy()\n",
    "    except AttributeError:\n",
    "        x_np, y_np = x, y\n",
    "    \n",
    "    x_sub, y_sub = x_np[:subset_size], y_np[:subset_size]\n",
    "    x_aug, y_aug = zip(*[add_random_heart(img, msk) for img, msk in zip(x_sub, y_sub)])\n",
    "    return np.array(x_aug), np.array(y_aug)\n",
    "\n",
    "# Augment training set with a subset of 5000 examples\n",
    "subset_size = 5000\n",
    "x_train_hearts, y_train_hearts = augment_dataset(x_train, y_train_digits, subset_size)\n",
    "\n",
    "# Augment test set with a subset of 1000 examples\n",
    "subset_size_test = 1000\n",
    "x_test_hearts, y_test_hearts = augment_dataset(x_test, y_test_digits, subset_size_test)\n",
    "\n",
    "# Create images with red heart overlay\n",
    "images_with_red_heart = [overlay_red(img, msk) for img, msk in zip(x_train_hearts, y_train_hearts)]\n",
    "\n",
    "# Plot: First row shows images with the white heart;\n",
    "# Second row shows images with the red heart overlay\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(4):  # Fixed to visualize 4 examples\n",
    "    # First row: Image with white heart\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(x_train_hearts[i].squeeze(), cmap='gray')\n",
    "    plt.title(\"White Heart\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Second row: Image with heart replaced by red\n",
    "    plt.subplot(2, 4, 4 + i + 1)\n",
    "    plt.imshow(images_with_red_heart[i])\n",
    "    plt.title(\"Red Heart\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-Tuning for 3 Classes\n",
    "We now fine-tune the segmentation model for three classes: background, digit, and heart, as opposed to 2 in phase 1. It starts by loading pretrained weights from Phase 1, skipping the final layer due to the new class configuration. This process helps the model adapt to the new task of segmenting hearts, digits, and background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phase2 = SegmentationModel(num_classes=3).to(device)\n",
    "\n",
    "# Load pretrained weights from Phase 1 (skip the mismatched final layer by using strict=False)\n",
    "# ATTENTION: This works only, if you are allowed to write to the disk (if the model is not download allrady, it has to write to disk)\n",
    "pretrained_dict = torch.load('pretrained_model_phase1.pth', map_location=device)\n",
    "model_dict = model_phase2.state_dict()\n",
    "\n",
    "# Filter out unnecessary keys (the final layer might not match)\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'out_conv' not in k}\n",
    "model_dict.update(pretrained_dict)\n",
    "model_phase2.load_state_dict(model_dict)\n",
    "\n",
    "optimizer_phase2 = optim.Adam(model_phase2.parameters(), lr=0.001)\n",
    "criterion_seg = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training Phase 2: Fine-tuning segmentation model with hearts (3 classes)...\")\n",
    "num_epochs_phase2 = 5\n",
    "\n",
    "# Create DataLoader for phase 2 training\n",
    "seg_hearts_train_ds = TensorDataset(torch.tensor(x_train_hearts).float(), torch.tensor(y_train_hearts).long())\n",
    "seg_hearts_train_loader = DataLoader(seg_hearts_train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs_phase2):\n",
    "    model_phase2.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, masks in seg_hearts_train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer_phase2.zero_grad()\n",
    "        outputs = model_phase2(imgs)\n",
    "        loss = criterion_seg(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer_phase2.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running_loss / len(seg_hearts_train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_phase2}] - Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the performance of a segmentation model trained with transfer learning (fine-tuned from Phase 1) against one trained from scratch on the same 3-class segmentation task. We initialize both models then train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, train a new segmentation model from scratch on the 3-class problem\n",
    "model_scratch = SegmentationModel(num_classes=3).to(device)\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs_scratch = 5\n",
    "loss_history_transfer = []\n",
    "loss_history_scratch = []\n",
    "\n",
    "# DataLoader for scratch training (reuse the hearts dataset)\n",
    "seg_hearts_train_loader = DataLoader(seg_hearts_train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"Training from scratch for comparison...\")\n",
    "for epoch in range(num_epochs_scratch):\n",
    "    # Fine-tuning transfer learning model\n",
    "    model_phase2.train()\n",
    "    running_loss_phase2 = 0.0\n",
    "    for imgs, masks in seg_hearts_train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer_phase2.zero_grad()\n",
    "        outputs = model_phase2(imgs)\n",
    "        loss = criterion_seg(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer_phase2.step()\n",
    "        running_loss_phase2 += loss.item() * imgs.size(0)\n",
    "    epoch_loss_phase2 = running_loss_phase2 / len(seg_hearts_train_loader.dataset)\n",
    "    loss_history_transfer.append(epoch_loss_phase2)\n",
    "\n",
    "    # Model trained from scratch\n",
    "    model_scratch.train()\n",
    "    running_loss_scratch = 0.0\n",
    "    for imgs, masks in seg_hearts_train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer_scratch.zero_grad()\n",
    "        outputs = model_scratch(imgs)\n",
    "        loss = criterion_seg(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer_scratch.step()\n",
    "        running_loss_scratch += loss.item() * imgs.size(0)\n",
    "    epoch_loss_scratch = running_loss_scratch / len(seg_hearts_train_loader.dataset)\n",
    "    loss_history_scratch.append(epoch_loss_scratch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_scratch}] Transfer Loss: {epoch_loss_phase2:.4f}, Scratch Loss: {epoch_loss_scratch:.4f}\")\n",
    "\n",
    "# Plot the training loss curves for comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history_transfer, 'b-', label='Transfer Learning')\n",
    "plt.plot(loss_history_scratch, 'r--', label='From Scratch')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Adding grid to the plot with dashed line style for the x-axis\n",
    "ax = plt.gca()  # Get the current axis\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Set x-ticks to integer values\n",
    "ax.set_xticks(range(num_epochs_scratch))  # Make sure the x-ticks are integers\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(idx):\n",
    "    \"\"\"\n",
    "    Visualize the original image and predicted segmentation mask (without the ground truth mask).\n",
    "    \"\"\"\n",
    "    sample_img = x_test_hearts[idx]  # shape: [1, 28, 28]\n",
    "    # Remove the ground truth mask, since you no longer need it\n",
    "    # gt_mask = y_test_hearts[idx]  # This line is no longer needed\n",
    "    \n",
    "    model_phase2.eval()\n",
    "    with torch.no_grad():\n",
    "        inp = torch.tensor(sample_img).unsqueeze(0).float().to(device)  \n",
    "        output = model_phase2(inp)  # output shape: [1, 3, 28, 28]\n",
    "        pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Create a 1-row, 2-column layout for the subplots (no ground truth mask)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "    \n",
    "    # Top row: \"Input MNIST with Heart\"\n",
    "    ax[0].imshow(sample_img.squeeze(), cmap='gray')\n",
    "    ax[0].set_title(\"Input MNIST with Heart\")\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Define a custom colormap for segmentation masks\n",
    "    cmap_seg = ListedColormap(['lightblue', 'green', 'red'])\n",
    "    \n",
    "    # Top row: \"Predicted Mask\"\n",
    "    ax[1].imshow(pred_mask, cmap=cmap_seg, vmin=0, vmax=2)\n",
    "    ax[1].set_title(\"Predicted Mask\")\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few random segmentation examples\n",
    "num_visualizations = 3\n",
    "for i in range(num_visualizations):\n",
    "    idx = random.randint(0, len(x_test_hearts)-1)\n",
    "    visualize_sample(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Image Denoising with Autoencoders\n",
    "\n",
    "In this section we build a convolutional autoencoder to perform image denoising.\n",
    "\n",
    "<img src=\"img/AE_denoise.png\" alt=\"Autoencorder\" width=\"800\"/>\n",
    "\n",
    "The autoencoder compresses the image into a latent space and then reconstructs the image from this representation.\n",
    "\n",
    "We will add Gaussian noise to the images and train the autoencoder to recover the clean images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.dec1(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "autoencoder = Autoencoder().to(device)\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to images\n",
    "noise_factor = 0.6\n",
    "x_train_noisy = x_train + noise_factor * torch.randn_like(x_train)\n",
    "x_test_noisy  = x_test + noise_factor * torch.randn_like(x_test)\n",
    "\n",
    "# Clip the values to be within [0,1]\n",
    "x_train_noisy = torch.clamp(x_train_noisy, 0., 1.)\n",
    "x_test_noisy  = torch.clamp(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Randomly select 'n' images from the dataset\n",
    "n = 5\n",
    "random_indices = torch.randperm(x_test.size(0))[:n]\n",
    "\n",
    "# Visualize some examples of noisy vs. clean images\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Plot noisy images first\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[random_indices[i]].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot clean images second\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(x_test[random_indices[i]].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Noisy vs. Clean MNIST Images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training the autoencoder for denoising...\")\n",
    "num_epochs_ae = 5\n",
    "ae_loss_history = []\n",
    "criterion_ae = nn.BCELoss()\n",
    "optimizer_ae = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader for the autoencoder (input: x_train_noisy, target: x_train)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "ae_train_ds = TensorDataset(x_train_noisy, x_train)\n",
    "ae_train_loader = DataLoader(ae_train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs_ae):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for noisy_imgs, clean_imgs in ae_train_loader:\n",
    "        noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        outputs = autoencoder(noisy_imgs)\n",
    "        loss = criterion_ae(outputs, clean_imgs)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        running_loss += loss.item() * noisy_imgs.size(0)\n",
    "    epoch_loss = running_loss / len(ae_train_loader.dataset)\n",
    "    ae_loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ae}] - Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot autoencoder training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ae_loss_history, label='Training Loss')\n",
    "plt.title('Autoencoder Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating model performance\n",
    "\n",
    "This code snippet evaluates the performance of the trained autoencoder on the test set. Here we also use the Mean Squared Error (MSE) to measure the average squared difference between the predicted (denoised) and actual (clean) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the autoencoder on the test set\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    decoded_imgs = autoencoder(x_test_noisy.to(device)).cpu()\n",
    "\n",
    "# Calculate Mean Squared Error between clean and denoised images\n",
    "mse = np.mean((x_test.cpu().numpy() - decoded_imgs.numpy()) ** 2)\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "\n",
    "# Choose 'n' random examples from the test set\n",
    "n = 6\n",
    "random_indices = np.random.choice(len(x_test), n, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Noisy input\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[idx].squeeze(), cmap='gray')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Noisy Input', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Denoised output\n",
    "    ax = plt.subplot(3, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[idx].squeeze(), cmap='gray')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Denoised Output', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Original image\n",
    "    ax = plt.subplot(3, n, i + 2 * n + 1)\n",
    "    plt.imshow(x_test[idx].squeeze(), cmap='gray')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Original Image', fontsize=14)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we explore the latent space of an autoencoder by defining an encoder that extracts the latent representation of input data. The encoder is constructed using two pooling layers from a pre-trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the latent space of the autoencoder\n",
    "# We define an encoder that returns the latent representation (after two pooling layers)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, autoencoder):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc1 = autoencoder.enc1  # Use enc1 directly\n",
    "        self.enc2 = autoencoder.enc2  # Use enc2 directly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)  # Applies Conv2d, ReLU, and MaxPool2d\n",
    "        x = self.enc2(x)  # Applies Conv2d, ReLU, and MaxPool2d\n",
    "        return x\n",
    "\n",
    "encoder = Encoder(autoencoder).to(device)\n",
    "\n",
    "# Randomly select 10 indices from the test dataset\n",
    "random_indices = np.random.choice(len(x_test), 10, replace=False)\n",
    "\n",
    "# Get encoded representations for the randomly selected test images\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_imgs = encoder(x_test[random_indices].to(device)).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(5):\n",
    "    # Original image\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(x_test[random_indices[i]].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Label: {y_test[random_indices[i]].item()}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Encoded representation: average over channels\n",
    "    ax = plt.subplot(2, 5, i + 6)\n",
    "    encoded_img = encoded_imgs[i].mean(axis=0)\n",
    "    plt.imshow(encoded_img, cmap='viridis')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Original Images vs. Their Encoded Representations (latent 7x7)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of different noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(images, noise_factor):\n",
    "    \"\"\"Add gaussian noise to images and clip to [0,1].\"\"\"\n",
    "    noisy = images + noise_factor * torch.randn_like(images)\n",
    "    return torch.clamp(noisy, 0., 1.)\n",
    "\n",
    "noise_levels = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "sample_idx = 25  # sample image index\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "# Display the original image\n",
    "plt.subplot(3, len(noise_levels) + 1, 1)\n",
    "plt.imshow(x_test[sample_idx].squeeze(), cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "for i, noise in enumerate(noise_levels):\n",
    "    noisy_sample = add_noise(x_test[sample_idx:sample_idx+1], noise)\n",
    "    \n",
    "    # Denoise the sample\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        denoised_sample = autoencoder(noisy_sample.to(device)).cpu()\n",
    "    \n",
    "    # Display noisy image\n",
    "    plt.subplot(3, len(noise_levels) + 1, i + 2)\n",
    "    plt.imshow(noisy_sample[0].squeeze(), cmap='gray')\n",
    "    plt.title(f'Noise: {noise}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display denoised image\n",
    "    plt.subplot(3, len(noise_levels) + 1, i + len(noise_levels) + 3)\n",
    "    plt.imshow(denoised_sample[0].squeeze(), cmap='gray')\n",
    "    plt.title('Denoised')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Compute and display MSE\n",
    "    mse_val = np.mean((x_test[sample_idx].cpu().numpy() - denoised_sample[0].cpu().numpy()) ** 2)\n",
    "    plt.subplot(3, len(noise_levels) + 1, i + 2*len(noise_levels) + 4)\n",
    "    plt.text(0.5, 0.5, f'MSE:\\n{mse_val:.4f}', ha='center', va='center', fontsize=12)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Denoising Performance Across Different Noise Levels', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
